# -*- coding: utf-8 -*-
"""movie_recommendation_system.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CXBswppylB6YucLWQGFb_IxbGssRe2qZ
"""

!pip install surprise

# All the Required Libraries
# These Libraries deals with Data Preprocessing
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import random
import seaborn as sns
# These Libraries deal with model importation training and Prediction
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from surprise import Reader, Dataset, SVD
from surprise.model_selection import cross_validate
from sklearn.model_selection import train_test_split
from surprise import accuracy
from surprise import KNNBasic
from surprise.model_selection import KFold

# Loading all the sub datasets to be used
sub_set1 = pd.read_csv('/content/tmdb_5000_credits.csv')
sub_set2 = pd.read_csv('/content/tmdb_5000_movies.csv')
sub_set3 = pd.read_csv('/content/ratings_small.csv')

"""**Viewing The Initial  Rows of the Sub Datasets**"""

# Viewing the Initial 5 rows of the  Movie credits
sub_set1.head()

# Viewing the Initial 5 rows of the  Movie dataset
sub_set2.head()

# Viewing the Initial 5 rows of the  Movie credits using a Reader Object
reader = Reader()
sub_set3.head()

"""**Exploratory Data Analysis**"""

# Renaming the Columns of  credits  dataset inorder to merge it with Movie  using common column id
sub_set1.columns = ['id','tile','cast','crew']
sub_set2= sub_set2.merge(sub_set1,on='id')

# viewing the improved movies dataset
sub_set2.head()

"""1.Data Visualization and Checking The Central Tendancy"""

#check the  Mean and 90th percentile of the Movie dataset
Mean= sub_set2['vote_average'].mean()
print(Mean)
per= sub_set2['vote_count'].quantile(0.9)
print(per)

# Creating a bar graph
plt.figure(figsize=(10, 10))
plt.bar(['Mean of vote averages (C)', '90th percentile of vote counts (m)'], [Mean, per], color=['b', 'g'])
plt.xlabel('Metric')
plt.ylabel('Value')
plt.title('Bar graph of mean of vote averages Mean and 90th percentile of vote counts per')
plt.grid(True)
plt.show()

# Create a pie chart
labels = ['Mean of vote averages (C)', '90th percentile of vote counts (m)']
sizes = [Mean, per]
colors = ['b', 'g']
explode = (0.1, 0)
plt.figure(figsize=(10, 10))
plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=140)
plt.title('Pie chart of mean of vote averages (Mean) and 90th percentile of vote counts (Per)')
plt.axis('equal')
plt.show()

# Release Date Analysis
sub_set2['release_date'] = pd.to_datetime(sub_set2['release_date'])
monthly_movie_counts = sub_set2.resample('M', on='release_date').size()
plt.figure(figsize=(16, 8))
monthly_movie_counts.plot()
plt.title('Monthly Movie Release Counts Over Time')
plt.xlabel('Release Date')
plt.ylabel('Number of Movies Released')
plt.show()

# Budget and Revenue Analysis
plt.figure(figsize=(12, 6))
sns.scatterplot(x='budget', y='revenue', data=sub_set2, color='orange')
plt.title('Budget vs. Revenue')
plt.xlabel('Budget')
plt.ylabel('Revenue')
plt.show()

"""**2.Demographic Filtering**

"""

# getting  the shape of  sub movie dataset that is greater than or equal  Mean
q_movies = sub_set2.copy().loc[sub_set2['vote_count'] >= Mean]
q_movies.shape

# Defining a function for weighted rating based on IMDB formula
def weighted_rating(x, m=per, C=Mean):
    v = x['vote_count']
    R = x['vote_average']
    # Calculation based on the IMDB formula
    return (v/(v+m) * R) + (m/(m+v) * C)

# Define a new feature 'score' and calculate its value with `weighted_rating()`
q_movies['score'] = q_movies.apply(weighted_rating, axis=1)

#Sort movies based on score calculated above
q_movies = q_movies.sort_values('score', ascending=False)

#Print the top 15 movies
q_movies[['title', 'vote_count', 'vote_average', 'score']].head(10)

# Creating a horizontal bar plot to visualize popular movies
pop= sub_set2.sort_values('popularity', ascending=False)
plt.figure(figsize=(12,4))

plt.barh(pop['title'].head(6),pop['popularity'].head(6), align='center',
        color='skyblue')
plt.gca().invert_yaxis()
plt.xlabel("Popularity")
plt.title("Popular Movies")

"""**Content Based Filtering**

A recommendation method called content-based filtering makes recommendations to a consumer based on the qualities of products they have previously liked. Content-based filtering algorithms examine attributes that users have found enjoyable in movies, including storyline keywords, actors, directors, and genre, in order to spot trends and preferences when it comes to movie suggestion. The algorithm suggests comparable films that the viewer would probably like based on these trends.

The content-based filtering algorithm, for example, will give recommendations for more comedic films precedence if the user has a history of watching and rating comedies well. This is because the system recognizes the user's apparent liking for humor and lighter amusement.

In addition, the algorithm will recommend movies with actors or directors that the user has indicated they enjoy watching.

Personalized movie suggestions and individualised tastes may be achieved with the use of content-based filtering. The technology can efficiently direct users toward films that match their interests by examining user preferences and seeing trends in their previous selections.


"""

# Display the overview of the first few movies
sub_set2['overview'].head()

# Text Vectorization
tfidf = TfidfVectorizer(stop_words = 'english')

sub_set2['overview'] = sub_set2['overview'].fillna('')

tfidf_matrix = tfidf.fit_transform(sub_set2['overview'])

tfidf_matrix.shape

"""**Text Vectorization**

In machine learning, text vectorization is the process of converting text input into numerical vectors. This is significant since machine learning techniques can only handle numerical data. These two broad categories of text vectorization techniques are:


Techniques that rely on counts: These techniques simply count how many times each word appears in a document. Two techniques that may be applied for this are TF-IDF and Bag-of-words (BoW).

Word embedding techniques include: Rather than focusing only on word counts, these approaches aim to capture the meaning of words and their relationships with one another. There are two ways to accomplish this: Word2Vec and GloVe.

Machine learning applications such as sentiment analysis, topic modeling, and natural language processing (NLP) rely on it.


"""

# Import TfidfVectorizer for text vectorization
cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)

#Construct a reverse map of indices and movie titles
indices = pd.Series(sub_set2.index, index = sub_set2['title']).drop_duplicates()

# Function that takes in movie title as input and outputs most similar movies

def get_recommendations(title, cosine_sim = cosine_sim):

    idx = indices[title]

    sim_scores = list(enumerate(cosine_sim[idx]))

    sim_scores = sorted(sim_scores, key = lambda x:x[1], reverse=True)

    sim_scores = sim_scores[1:11]

    movie_indices = [i[0] for i in sim_scores]

    return sub_set2['title'].iloc[movie_indices]

# Getting Recommendation
get_recommendations('Stolen')

get_recommendations('Plastic')

"""Credits, Genres and Keywords Based Recommender"""

# Parse the stringified features into their corresponding python objects
from ast import literal_eval

features = ['cast', 'crew', 'keywords', 'genres']
for feature in features:
    sub_set2[feature] = sub_set2[feature].apply(literal_eval)

# Define functions to extract directors  from features
def get_director(x):
    for i in x:
        if i['job'] == 'Director':
            return i['name']
    return np.nan

# Define functions to get list of names from features

def get_list(x):
    if isinstance(x, list):
        names = [i['name'] for i in x]

        if len(names) > 3:
            names = names[:3]
        return names


    return []

sub_set2['director'] = sub_set2['crew'].apply(get_director)

features = ['cast', 'keywords', 'genres']
for feature in features:
    sub_set2[feature] = sub_set2[feature].apply(get_list)

# Print the new features of the first 3 films
sub_set2[['title', 'cast', 'director', 'keywords', 'genres']].head(3)

# Function to convert all strings to lower case and strip names of spaces
def clean_data(x):
    if isinstance(x, list):
        return [str.lower(i.replace(" ","")) for i in x]

    else:

        if isinstance(x , str):
            return str.lower(x.replace(" ",""))
        else:
            return ''

# Apply clean_data function to your features.
features = ['cast', 'keywords', 'director', 'genres']

for feature in features:
    sub_set2[feature] =sub_set2[feature].apply(clean_data)

def create_soup(x):
    return ' '.join(x['keywords']) + ' ' + ' '.join(x['cast']) + ' ' + x['director'] + ' ' + ' '.join(x['genres'])
sub_set2['soup'] = sub_set2.apply(create_soup, axis=1)

# Import CountVectorizer and create the count matrix
count = CountVectorizer(stop_words='english')
count_matrix = count.fit_transform(sub_set2['soup'])

# Compute the Cosine Similarity matrix based on the count_matrix
cosine_sim2 = cosine_similarity(count_matrix, count_matrix)

# Reset index of our main DataFrame and construct reverse mapping as before
sub_set2 = sub_set2.reset_index()
indices = pd.Series(sub_set2.index, index=sub_set2['title'])

# Prediction Corner
Movie=input("Enter Movie Name to get Other Recommendations:")
get_recommendations(Movie, cosine_sim2)

"""Collaborative Filtering"""

data = Dataset.load_from_df(sub_set3[['userId', 'movieId', 'rating']], reader)

svd = SVD()
cross_validate(svd, data, measures=['RMSE', 'MAE'])

trainset = data.build_full_trainset()
svd.fit(trainset)

sub_set3[sub_set3['userId']==2]

svd.predict(1, 302, 3)

from surprise import Dataset
from surprise.model_selection import train_test_split

# Here we load the dataset
reader = Reader()
data = Dataset.load_from_df(sub_set3[['userId', 'movieId', 'rating']], reader)

# Split the data into training and testing sets
trainset, testset = train_test_split(data, test_size=0.2, random_state=42)

# Performing A/B Test
# Comparing SVD vs KNNBasic
a1 = svd
a2 = KNNBasic()

# Function for A/B Test
def ab_test(algorithm1, algorithm2, trainset, testset):
    random.seed(42)  # Set seed for reproducibility

    # Training SVD on the trainset
    a1.fit(trainset)

    # Training KNN on the trainset
    a2.fit(trainset)

    # Evaluating both algorithms
    predict1 = a1.test(testset)
    predict2 = a2.test(testset)

    # Comparing both using RMSE and MAE
    rmse1 = accuracy.rmse(predict1)
    rmse2 = accuracy.rmse(predict2)
    mae1 = accuracy.mae(predict1)
    mae2 = accuracy.mae(predict2)

    # Print the results
    print(f'RMSE for SVD: {rmse1}')
    print(f'RMSE for KNNBasic: {rmse2}')
    print(f'MAE for SVD: {mae1}')
    print(f'MAE for KNNBasic: {mae2}')

    return rmse1, rmse2, mae1, mae2

# Running A/B test
rmse_svd, rmse_knn, mae_svd, mae_knn = ab_test(a1, a2, trainset, testset)

# Compare the results and print which is better
if rmse_svd < rmse_knn:
    print('SVD is better in RMSE.')
else:
    print('KNNBasic is better in RMSE.')

if mae_svd < mae_knn:
    print('SVD is better in MAE.')
else:
    print('KNNBasic is better in MAE.')